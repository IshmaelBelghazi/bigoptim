#+TITLE: BigOptim -- Large Scale Finite Sums Cost functions Optimization for R

[[https://travis-ci.org/IshmaelBelghazi/bigpoptim][https://travis-ci.org/IshmaelBelghazi/bigoptim.svg]]

* Description
BigOptim is an R package that implements the Stochastic Average Gradient(SAG)[1] optimization method. For strongly convex problems, SAG achieves batch gradient descent convergence rates while keeping the iteration complexity of stochastic gradient descent. This allows for efficient training of machine learning algorithms with convex cost functions.
* Setup
#+BEGIN_SRC R
install.packages("devtools")
devtools::install_github("hadley/devtools")  ## Optional
devtools::install_github("IshmaelBelghazi/bigoptim")
#+END_SRC

* Example: Fit with Linesearch
#+BEGIN_SRC R
## Loading Data set
data(covtype.libsvm)
## Normalizing Columns and adding intercept
X <- cbind(rep(1, NROW(covtype.libsvm$X)), scale(covtype.libsvm$X))
y <- covtype.libsvm$y
y[y == 2] <- -1
## Setting seed
#set.seed(0)
## Setting up problem
maxiter <- NROW(X) * 10  ## 10 passes throught the dataset
lambda <- 1/NROW(X) 
sag_ls_fit <- sag_fit(X=X, y=y, lambda=lambda,
                      Li=Li, stepSizeType=1,
                      maxiter=maxiter, 
                      tol=1e-08, family="binomial", fit_alg="linesearch",
                      standardize=FALSE)
## Getting weights
weighs <- coef(sag_ls_fit)
## Getting cost
cost <- get_cost(sag_ls_fit)
#+END_SRC
* Example: Demo -- Monitoring gradient norm
#+BEGIN_SRC R
data(monitoring_training)
#+END_SRC
#+CAPTION: Gradient norm after each effective pass through the dataset
#+NAME: gradien_monitoring
[[misc/readme/grad_norm_covtype.png]]
* References

[1] Mark Schmidt, Nicolas Le Roux, and Francis Bach. Minimizing Finite Sums with the Stochastic Average Gradient. arXiv:1309.2388 [cs, math, stat], September 2013. arXiv: 1309.2388. [ [[http://ishmaelbelghazi.bitbucket.org/SAG_proposal/proposal_IshmaelB_bib.html#schmidt_minimizing_2013][bib]] | [[http://arxiv.org/abs/1309.2388][http]] ] 

  
